Developing a Real-Time Social Listening and Intelligence Platform on Google Cloud for Strategic Business Insights.
1	ABSTRACT
TODO 
A concise summary of your research, methodology, key findings, and contributions.
2	ACKNOWLEDGEMENT
TODO
Thank my supervisor, Sense Worldwide, and everyone who supported me.
3	TABLE OF CONTENTS
4	LIST OF TABLES
5	LIST OF FIGURES
6	INTRODUCTION
6.1	BACKGROUND AND CONTEXT
In today's rapidly evolving business landscape, understanding consumer sentiment, market trends, and emerging opportunities is paramount for strategic decision-making. The pervasive nature of social media platforms has transformed them into a vast, dynamic repository of unfiltered public opinion, discussions, and real-time insights. Businesses and consultancies, in particular, recognize the immense value held within this unstructured data, as it offers unparalleled access to authentic perspectives that can drive innovation, refine strategies, and anticipate market shifts. The ability to effectively "listen" to these digital conversations has become a critical competitive differentiator, allowing organizations to maintain relevance and adapt swiftly to changing consumer needs.
6.2	SENSE WORLDWIDE: A CASE CONTEXT
Sense Worldwide, a London-based strategic innovation consultancy established in 1999, pioneers co-creation by introducing unconventional perspectives into the innovation process. The company focuses on understanding consumer behaviours, needs, and desires to help client companies create and improve innovative products, services, and experiences. A key strength lies in their immersion in diverse cultures and contexts, enabling brands to tailor offerings to specific audiences.
Sense Worldwide acts as the gatekeeper to The Sense Network, a global community of over 6,000 creators from over 1,300 cities, described as 'one of the earliest web-based communities' (Donkin, 2010). Through these 'outliers, misfits, rebels and the crazy ones' (Sense Worldwide, 2024), Sense Worldwide harnesses cognitive diversity to understand future trends and develop breakthrough strategies. Their process involves designing survey-based activities to gather in-depth feedback, followed by qualitative analysis to provide clients with actionable recommendations. Sense Worldwide has delivered over 500 projects for clients including PepsiCo, Nike, and Samsung, and has been recognized by Nesta (2011) as one of the 'Open 100' companies that pioneered open innovation.
6.3	PROBLEM STATEMENT
However, extracting actionable intelligence from this deluge of social data presents significant challenges. Traditional market research methods, while valuable for depth, are often time-consuming, expensive, and may not capture the ephemeral nature of online discourse. Manual social media monitoring, conversely, is inherently inefficient, labor-intensive, and lacks the scalability to process large volumes of diverse content. For strategy consultancies like Sense Worldwide, a reliance on such methods can hinder their agility in providing timely and comprehensive "points of view" to clients, potentially delaying crucial strategic insights and incurring substantial operational costs. There is a clear and pressing need for more efficient, systematic, and cost-effective solutions to harness social media intelligence.
6.4	RESEARCH QUESTIONS:
It is within this context that this dissertation aims to address the aforementioned challenges by developing a pragmatic and advanced solution. The central research question guiding this study is:
How can a Google Cloud-based social listening platform, designed for cost-effective and on-demand execution, be developed to effectively gather and analyze social media data, thereby augmenting traditional market research and accelerating the generation of strategic consumer insights for a consultancy?
To comprehensively answer this central question, the following sub-questions will be investigated:
•	What architectural design and data pipeline components are optimal for a scalable and cost-effective social media data ingestion and processing system built on Google Cloud Platform?
•	How can Natural Language Processing (NLP) techniques, particularly embeddings, topic modeling, and Large Language Models, be leveraged within this platform to identify key themes, assess sentiment, and derive interpretable insights from unstructured social media data?
•	How does the implementation of such a tool enhance the efficiency, depth of insight generation, and cost-efficiency when integrated into existing market research workflows within a strategy consultancy?
This project thus aims to practically demonstrate the design, implementation, and evaluation of such a solution, contributing to both academic knowledge on cloud-native analytical systems and providing a replicable blueprint for organizations seeking cost-efficient social intelligence.
7	LITERATURE REVIEW
This chapter provides a comprehensive review of the academic and industry literature relevant to the development of a social listening and intelligence platform. It synthesizes existing knowledge across social media analytics, market research, data engineering, cloud computing, and natural language processing. By examining established methodologies and current solutions, this review aims to establish the theoretical foundation for the proposed system and clearly articulate the research gap addressed by this dissertation.
7.1	INTRODUCTION TO SOCIAL LISTENING AND SOCIAL MEDIA ANALYTICS
Social media platforms have evolved into indispensable channels for communication, commerce, and information dissemination, generating an unprecedented volume of user-generated content. This phenomenon has given rise to the field of social media analytics, which focuses on collecting, analyzing, and interpreting data from these platforms (Fan & Gordon, 2014). Within this domain, social listening and social media monitoring are often used interchangeably, but they represent distinct levels of analysis. Social media monitoring typically involves tracking mentions of keywords, brands, or topics (e.g., counting mentions, tracking reach). In contrast, social listening transcends simple tracking; it involves a deeper analysis of the conversations to understand the 'why' behind online mentions, identify underlying sentiments, emerging trends, and actionable insights (Culnan et al., 2010; Weinberg & Pehlivan, 2011).
The importance of social listening for businesses is multifaceted. It enables real-time reputation management, competitor analysis, customer service improvements, and product innovation by providing direct access to unfiltered consumer opinions (Mangold & Faulds, 2009). However, harnessing this value presents challenges related to the sheer volume, velocity, and unstructured nature of social media data, necessitating sophisticated analytical tools and methodologies.
7.2	MARKET RESEARCH AND CONSUMER INSIGHTS
Traditional market research methodologies, such as surveys, focus groups, and in-depth interviews, are foundational for understanding consumer behavior, needs, and desires. They provide rich qualitative depth and controlled quantitative data (Malhotra, 2019). However, these methods can be time-consuming, expensive, may suffer from self-reporting biases, and often capture static snapshots rather than dynamic, unsolicited opinions.
Social listening directly augments traditional market research by offering a complementary data stream. It provides unsolicited, real-time insights into consumer conversations as they naturally occur, revealing unarticulated needs and authentic sentiment that might not surface in structured research settings (Kaplan & Haenlein, 2010). The integration of social listening allows organizations to move beyond mere data collection to generating consumer insights—the deep, actionable understandings of consumer motivations and behaviors that inform strategic decisions and drive innovation (Neff, 2007). This synergy between traditional and social research methodologies enables a more holistic and dynamic understanding of the market landscape.
7.3	DATA ENGINEERING PRINCIPLES FOR SOCIAL MEDIA DATA
Effective social listening relies on robust data engineering principles to manage the unique characteristics of social media data: volume, velocity, variety, and veracity (the '4 Vs' of Big Data) (Laney, 2001). Data pipelines are fundamental constructs for handling this. A typical data pipeline involves stages such as data ingestion, transformation, storage, and processing (Kamburugamuve et al., 2018).
•	Data Ingestion: Involves collecting raw data from various sources. For social media, this often requires interacting with platform-specific APIs (if available) or employing web scraping techniques. Web scraping, while powerful, necessitates careful consideration of legal, ethical, and platform Terms of Service (ToS) compliance, as well as overcoming technical challenges like anti-bot measures and dynamic content rendering (Holzmann, 2017).
•	Data Transformation (ETL/ELT): Raw social media data is often messy, unstructured, and hierarchical. Transformation involves cleaning, parsing, flattening nested structures, and standardizing schemas to prepare data for analytical models. ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform) paradigms are applied to ensure data quality and consistency (Kimball & Ross, 2013).
•	Data Storage: Requires scalable and flexible solutions capable of handling diverse data types, from semi-structured JSON to unstructured text. Cloud data warehouses or NoSQL databases are frequently employed.
Designing resilient and efficient pipelines for social media data requires strategic choices to manage resource consumption and ensure data integrity in asynchronous environments (Zikopoulos et al., 2015).
7.4	CLOUD COMPUTING FOR DATA SOLUTIONS 
Cloud computing has become instrumental in handling Big Data workloads due to its scalability, flexibility, and cost-effectiveness (Buyya et al., 2009). Platform-as-a-Service (PaaS) and Software-as-a-Service (SaaS) models offer managed services that abstract away infrastructure complexities, allowing developers to focus on application logic.
Google Cloud Platform (GCP), in particular, offers a comprehensive suite of interconnected services highly suitable for data analytics workloads. Its strengths include:
•	Serverless Computing: Services like Cloud Functions enable event-driven, pay-per-execution models, ideal for intermittent or on-demand tasks, significantly reducing idle compute costs (Jamison et al., 2021).
•	Scalable Data Warehousing: BigQuery provides a serverless, highly scalable, and cost-effective data warehouse solution, well-suited for ingesting, storing, and querying petabytes of data without infrastructure management (Google Cloud, n.d.). Its pay-per-query pricing model aligns with cost-efficient analytics.
•	Integrated ML/AI Capabilities: GCP integrates machine learning directly into its data services (e.g., BigQuery ML), or offers dedicated AI platforms (e.g., Vertex AI) with access to powerful pre-trained models. This integration streamlines the application of advanced analytics.
•	Managed Messaging: Services like Pub/Sub ensure robust and asynchronous communication between loosely coupled components, crucial for resilient data pipelines (Google Cloud, n.d.b).
The strategic adoption of such cloud-native services provides an agile and economically viable foundation for building complex analytical systems.
7.5	NATURAL LANGUAGE PROCESSING (NLP) FOR TEXT ANALYSIS
Natural Language Processing (NLP) is a field of Artificial Intelligence that enables computers to understand, interpret, and generate human language (Jurafsky & Martin, 2009). For social listening, NLP is indispensable for extracting meaning from vast quantities of unstructured text.
•	Text Embeddings: Modern NLP heavily relies on text embeddings, which are dense numerical vector representations of words, phrases, or entire documents. These embeddings capture semantic meaning and contextual relationships, allowing mathematical operations (e.g., similarity calculations, clustering) on textual data. Models like Word2Vec, BERT, and Google's text-embedding-004 (a foundation model) generate these vectors, capable of processing raw text and discerning nuances in language (Mikolov et al., 2013; Devlin et al., 2019).
•	Topic Modeling: A statistical NLP technique used to discover abstract 'topics' or themes that occur in a collection of documents (Blei, 2012). K-Means clustering, when applied to text embeddings, is an effective method for topic modeling, grouping semantically similar documents into clusters representing distinct themes (Ghasemi et al., 2021).
•	Sentiment Analysis: An NLP subfield focused on determining the emotional tone (positive, negative, neutral) and intensity (magnitude) expressed in text. It is vital for gauging public opinion, brand perception, and customer satisfaction (Liu, 2012). Services like Google Cloud Natural Language API provide pre-trained models for this purpose.
•	Large Language Models (LLMs): Advanced AI models trained on massive text datasets, capable of understanding, generating, and interpreting human language with remarkable fluency and coherence (Brown et al., 2020). LLMs like Google's Gemini can be leveraged to interpret statistical outputs (e.g., topic clusters) by generating human-readable labels and summaries, bridging the gap between machine analysis and human understanding of insights.
7.6	EXISTING SOCIAL LISTENING TOOLS AND SOLUTIONS
The market for social listening tools is diverse, ranging from comprehensive commercial platforms to open-source frameworks.
•	Commercial Tools: Platforms like Brandwatch, Sprout Social, and Talkwalker offer extensive features including real-time monitoring, sentiment analysis, influencer identification, and customizable dashboards. They provide robust functionality and managed services, often suitable for large enterprises. However, these solutions typically come with high subscription costs, may offer limited customization for niche analytical needs, and can lead to vendor lock-in, posing a barrier for smaller consultancies or specific project-based requirements (Rapp & Schlerf, 2019).
•	Open-Source Frameworks: Alternatives involve leveraging open-source libraries and APIs (e.g., Python libraries for scraping like Scrapy, NLP libraries like NLTK, spaCy). While offering maximum customization and avoiding licensing fees, this approach demands significant in-house development expertise, time investment for building and maintaining pipelines, and managing infrastructure scalability (Gensim, n.d.). It also requires careful handling of platform API rate limits and Terms of Service.
The existing landscape suggests a trade-off between the high cost and limited flexibility of commercial platforms versus the high development overhead of purely open-source solutions for bespoke social intelligence needs.
7.7	IDENTIFYING THE RESEARCH GAP
The preceding literature review highlights the critical role of social listening and advanced NLP in deriving consumer insights. While traditional market research and manual social media monitoring face limitations in efficiency and scale, and existing commercial tools often incur high costs or lack customization, a clear gap exists for a flexible, cost-effective, and on-demand social listening platform specifically tailored to the agile needs of strategic consultancies.
Existing solutions either provide broad, expensive services (commercial) or require extensive, specialized in-house development (open-source). There is a need for a solution that:
•	Leverages the scalability and cost-efficiency of cloud-native, serverless architectures.
•	Provides on-demand, targeted data acquisition rather than continuous, potentially costly, monitoring.
•	Integrates advanced analytical techniques, including modern text embeddings and Generative AI for interpretable insights, directly into the data workflow.
•	Offers a customizable, in-house controlled alternative to expensive third-party platforms.
This dissertation aims to address this gap by designing, implementing, and evaluating such a solution, thereby contributing a practical framework for consultancies to harness social media intelligence efficiently and economically.
8	METHODOLOGY AND SYSTEM DESIGN (5549 WORDS)
8.1	RESEARCH APPROACH
This dissertation adopts a Design Science Research (DSR) approach, complemented by principles of Applied Research. DSR is a problem-solving paradigm focused on creating and rigorously evaluating innovative artifacts in real-world contexts (Hevner et al., 2004). It is prescriptive, aiming to build and assess solutions to relevant problems, thereby generating both practical and theoretical contributions. The typical DSR process involves an iterative cycle of problem identification, artifact design, development, demonstration, and evaluation (Peffers et al., 2007).
This methodology is uniquely suited to the project's goal of developing a functional social listening and intelligence platform for Sense Worldwide because:
•	Problem-Solving Focus: DSR explicitly aims to solve practical problems. The identified challenge of costly, time-consuming manual social media research directly aligns with this.
•	Artifact Creation: The dissertation's core outcome is a tangible, working software solution—the social listening tool—which is the 'artifact' central to DSR.
•	Utility and Evaluation: DSR emphasizes artifact utility and effectiveness in its intended environment. This aligns perfectly with creating a tool for Sense Worldwide to gain insights and reduce costs. The developed solution will undergo rigorous evaluation to assess its real-world utility and effectiveness, consistent with DSR's focus on practical impact.
•	Knowledge Contribution: Beyond tool building, DSR contributes knowledge by demonstrating how such an artifact can be successfully designed, built, and deployed to solve specific problems (e.g., leveraging cloud-native AI for cost-effective market intelligence).
8.2	SYSTEM ARCHITECTURE DESIGN
This section details the architectural design of the developed social listening and intelligence platform. The system is structured into interconnected logical layers, each enhancing modularity, scalability, and maintainability. This layered approach strategically leverages various Google Cloud services and external APIs to achieve a cost-efficient, on-demand solution for generating business insights. Figure 3.1 depicts a high-level overview of the system's component architecture and interactions.
 
8.2.1	Presentation and Control Layer
This layer serves as the primary interface for user interaction and system orchestration.
•	Google Sheets (Control Panel UI): Acts as the intuitive, low-code user interface. Consultants interact with sheets to input search queries, specify social media sources, and select data for processing. This choice leverages existing user familiarity and significantly reduces frontend development costs.
•	Google Apps Script (Automation & Trigger): Embedded within Google Sheets, Apps Script provides automation logic. It translates user actions (e.g., button clicks) into programmatic triggers, initiating Cloud Function API calls and orchestrating workflow across system layers.
8.2.2	Orchestration & Ingestion Layer
This layer coordinates external data calls, handles direct data ingestion, and prepares raw data for storage.
•	Google Cloud Functions (Orchestration & Data Delivery): These serverless functions initiate Brightdata API calls for search and scraping. Triggered by new Pub/Sub messages, they also process and deliver scraped content to the correct BigQuery destination tables using associated job metadata (e.g., snapshot_id, dataset_id). Their pay-per-execution model significantly contributes to cost-efficiency.
•	Google Pub/Sub (Messaging Queue): This managed, asynchronous service acts as the direct delivery endpoint for Brightdata's scraped data. It reliably queues incoming messages, ensuring secure and decoupled transfer of raw content for Cloud Function processing.
•	Brightdata (External APIs: SERP, Scrapers): This external service provides robust web scraping capabilities. It is utilized to retrieve search engine results (via SERP API) and extract detailed social media content (via specialized Web Scrapers). Brightdata's managed service model reduces in-house overhead and cost.
8.2.3	Data Storage Layer
This layer provides the central repository for all data within the system, designed for scalability and analytical efficiency.
•	Google BigQuery (Data Warehouse): BigQuery serves as the system's core serverless and highly scalable data warehouse. It stores raw SERP results, raw scraped social media data (from Reddit and Quora), and all subsequent processed data. BigQuery's pay-per-query pricing model for analysis and storage costs based on data volume are pivotal to the overall low-cost design, particularly for an on-demand system.
o	Data Transformation and Unification: Raw, platform-specific data ingested from Reddit and Quora is transformed and unified into a unified_social_content_items BigQuery View. This view acts as a logical, standardized dataset, flattening hierarchical content (e.g., separating each comment/reply into its own row) and standardizing schema across sources. This transformation is executed dynamically when the view is queried by downstream components (such as BigQuery ML), ensuring data freshness and avoiding redundant physical storage.
8.2.4	Raw Data Model Design
This section details the schema of tables designed to store raw, unprocessed data as it is ingested into BigQuery. This foundational layer of the data model is crucial for preserving the original structure and content received directly from external data sources, serving as the immutable input for subsequent transformations. The tables outlined here capture the initial metadata from search engine results and the scraped content from social media platforms.
8.2.4.1	serp_search Table
The serp_search table is designed to track the metadata associated with each search engine results page (SERP) query initiated by the system. This allows for an audit trail of executed searches and their high-level characteristics.
 
8.2.4.2	serp_result Table
The serp_result table stores the individual search results (links and their associated details) retrieved from each SERP query. It links back to the serp_search table via serp_request_id, enabling a complete picture of what links were found for a given search. 

 
8.2.4.3	scrape_job Table 
Following the acquisition of relevant social media links (either from SERP results or directly provided by the user), the system initiates dedicated scraping jobs. The scrape_job table is designed to track and manage these individual batch scraping requests, providing an essential record of all data collection tasks.
A key field, dataset_id, is used to internally identify the specific Brightdata web scraper configured for the job (e.g., the Reddit scraper versus the Quora scraper). The snapshot_id and dataset_id stored in this table are crucial for the Cloud Functions in the Orchestration & Ingestion layer to correctly identify, process, and route the incoming scraped data to its appropriate destination (e.g., reddit_data or quora_data tables).
 
8.2.4.4	reddit_data Table
The reddit_data table serves as the primary repository for raw, unstructured social media content scraped from the Reddit platform. Data is delivered to this table by the Orchestration & Ingestion layer, preserving the original, hierarchical structure of Reddit posts, including nested comments and replies, before further transformation.
For brevity, a representative subset of key fields from the reddit_data table schema is presented below, highlighting essential content and metadata attributes. The full schema encompasses additional platform-specific details.
 
8.2.4.5	quora_data Table
Similarly, the quora_data table stores the raw social media content extracted from the Quora platform. This table captures the structure of Quora questions and their associated answers or comments, serving as the raw input before schema unification.

A representative subset of key fields from the quora_data table schema is provided below. As with reddit_data, additional platform-specific fields are present in the full schema for comprehensive data capture.
 
8.2.5	Data Processing & Analytics Layer
This layer is responsible for transforming raw and unified data into actionable insights, leveraging advanced machine learning and artificial intelligence capabilities. BigQuery ML is the central analytical engine within this layer, seamlessly integrating with external AI services through its remote model capabilities. It enables the execution of machine learning models directly on data stored in BigQuery, significantly reducing data egress costs and simplifying the ML pipeline. Its functionalities include:
•	Embeddings Generation: Text embeddings are generated by BigQuery ML, invoking the text-embedding-004 foundation model served by Google Vertex AI. These embeddings are periodically updated via a scheduled query that processes only newly added data from the unified_social_content_items view, ensuring efficiency and cost-optimization by avoiding redundant recalculations.
•	K-Means Clustering: K-Means clustering is performed on the generated embeddings within BigQuery ML to identify distinct topics or themes, utilizing cosine distance for similarity.
•	Sentiment Analysis: Sentiment scoring of social media content is performed by BigQuery ML, directly invoking the Google Cloud Natural Language API for analysis.
•	LLM Calls (Gemini for Topic Interpretation): BigQuery ML also orchestrates calls to the Gemini Large Language Model (LLM), served by Google Vertex AI, for the generation of interpretable topic titles and summaries by analyzing representative documents within each identified cluster.
•	Google Cloud Function (UMAP Processing): A dedicated Cloud Function executes a Python script to perform UMAP (Uniform Manifold Approximation and Projection) dimensionality reduction on the text embeddings. UMAP is chosen for its ability to preserve both local and global data structures, essential for visually representing topic clusters in the dashboard.
8.2.6	Processed Data Model Design
This section details the schema of tables and views that store data after it has been processed and transformed by the analytical layer. These structures represent the intermediate and final outputs of the data processing pipeline, designed for efficient storage and retrieval of analytical features and insights.
8.2.6.1	unified_social_content_items View
The unified_social_content_items BigQuery View represents the culmination of the data transformation process, providing a clean, flattened, and standardized dataset from various social media sources. This view is a logical construct that does not physically store data, but dynamically executes its underlying SQL query whenever accessed. It serves as the single, consistent input for all subsequent data processing and analytical tasks, including embeddings generation, sentiment analysis, and topic modeling.
The design of this view specifically addresses the challenges posed by the diverse and often hierarchical structures of raw social media data by:
•	Flattening Hierarchical Content: Decomposing nested structures (like posts with multiple comments and replies) into individual, distinct rows, ensuring each piece of content can be treated as a standalone document for analysis.
•	Standardizing Schema: Unifying platform-specific data fields (from Reddit and Quora) into a common, consistent schema, enabling seamless, source-agnostic processing downstream.
The final schema for the unified_social_content_items view is as follows:

 
8.2.6.1.1	Key Transformation Logic
The unified_social_content_items view employs several sophisticated SQL techniques to achieve its standardized and flattened structure:
•	Common Table Expressions (CTEs): The view utilizes multiple CTEs (e.g., reddit_posts_base, reddit_comments_base, quora_questions_base) to modularize the processing of raw data from each platform and content type (posts, comments, replies).
•	UNNEST Operator: The UNNEST operator is extensively used to flatten the nested RECORD and REPEATED fields (such as comments and replies in the raw tables). This transforms hierarchical data into a flat, row-per-content-item structure, essential for uniform text analysis.
•	Stable content_item_id Generation: To ensure each individual piece of content across all sources and types has a unique and deterministic identifier, custom IDs are generated using CONCAT and FARM_FINGERPRINT. This technique creates a stable hash based on core content attributes (e.g., post_id, author, timestamp, text content), ensuring re-scraped identical items retain the same ID, which is critical when native platform IDs are inconsistent or absent for nested comments/replies.
•	UNION ALL Operator: Data from the flattened CTEs for Reddit (posts, comments, replies) and Quora (questions, answers, replies) is combined using UNION ALL to create the final unified dataset, ensuring all content adheres to the unified_social_content_items schema.
•	ROW_NUMBER() for Deduplication/Version Control: The ROW_NUMBER() window function, partitioned by content_item_id and ordered by content_timestamp (or record_load_timestamp), is applied to select the most recent or unique version of each content item. This is crucial for handling potential duplicates that might arise from re-scraping or data ingestion processes, ensuring that downstream analysis operates on a clean dataset.
The full SQL definition for the unified_social_content_items view is provided in Appendix A.1.
8.2.6.2	embeddings_cache Table
To optimize computational resources and ensure efficient access to key analytical features, a dedicated embeddings_cache table is maintained. This table serves as a persistent store for pre-computed text embeddings and sentiment scores for individual content items from the unified_social_content_items view. Its design prevents redundant recalculations of these resource-intensive features, directly supporting the system's cost-efficiency and performance.
The schema for the embeddings_cache table is as follows:


 
8.2.6.3	document_topic_assignments Table
This table stores the direct output of the K-Means clustering process. For each analytical run, it links individual content items from the unified_social_content_items view to their assigned topic or cluster. This enables granular analysis of which content belongs to which discovered theme.

 
8.2.6.4	kmeans_run Table
The kmeans_run table serves as a metadata catalog for each execution of the K-Means clustering process. It records essential parameters and identifiers for every clustering analysis performed, allowing for traceability and comparison of different topic modeling runs. The run_id in this table acts as a foreign key for other processed data tables (document_topic_assignments, topic_labels, document_umap_coordinates).

 
8.2.6.5	topic_labels Table
This table stores the human-interpretable labels and detailed descriptions for each identified topic, generated through the integration with Google's Gemini LLM. This crucial step transforms raw cluster IDs into actionable insights for consultants.
 
8.2.6.6	document_umap_coordinates Table
This table stores the 2D or 3D coordinates for each content item (unified_id) after UMAP dimensionality reduction. These coordinates are specifically designed for visualizing topic clusters and content relationships in a lower-dimensional space within the Looker Studio dashboard.
 
8.2.7	Visualization Layer
This layer is dedicated to presenting the processed data and insights in an accessible and interactive format for the end-users.
•	Looker Studio (Specialized Reports): Looker Studio is utilized as the primary visualization tool. It connects directly to the processed tables in BigQuery, enabling the creation of interactive dashboards and specialized reports. These dashboards visualize topic distributions, UMAP cluster plots, sentiment trends, and crucially, display the LLM-generated topic titles and summaries, providing comprehensive and actionable insights to Sense Worldwide consultants. Each analysis run provides a unique run ID to access its specific report.
8.3	DATA COLLECTION STRATEGY
This section outlines the strategy employed for collecting the raw social media data necessary for the platform's development and analytical capabilities. The primary objective was to acquire publicly accessible, text-rich social media content that is suitable for Natural Language Processing (NLP) and capable of yielding deep consumer insights for strategic consulting purposes. The subsequent subsections detail the rationale behind the selection of specific data sources, the types of data collected, the methods utilized for web scraping, and the ethical and legal considerations observed.
8.3.1	Selection of Data Sources
The primary social media data sources selected for this project are Reddit and Quora. This selection was guided by several key criteria aligned with the project's objectives and the specific needs of Sense Worldwide:
•	Rich Textual Content: Both platforms are highly text-centric, providing extensive user-generated content (posts, comments, answers) that is ideal for textual analysis using Natural Language Processing (NLP) techniques like embeddings, topic modeling, and sentiment analysis.
•	Diverse Perspectives and Niche Communities: Reddit, with its vast array of subreddits, offers access to highly engaged communities discussing diverse and often niche topics, providing granular insights into specific interests, pain points, and emerging trends. Quora, as a question-and-answer platform, provides direct insights into user questions, problems, and collective knowledge, often with more structured, opinion-based answers.
•	Public Accessibility: Content on both platforms is largely publicly available, facilitating collection through legitimate scraping methods.
•	Relevance to Consultancy Insights: The nature of discussions on Reddit (e.g., product feedback, brand sentiment, grassroots community trends) and Quora (e.g., common problems, solutions, expert opinions) directly lends itself to the type of consumer insights and "points of view" that strategy consultancies seek.
8.3.2	Data Types Collected
The data collection strategy focused on extracting comprehensive and granular information from selected social media platforms to facilitate rich textual analysis and insight generation. For each content item (posts, questions, comments, and replies), the system aimed to capture fields critical for both identification and analytical depth.
The specific types of data collected from both Reddit and Quora include:
•	Content Identification: Unique identifiers for each post/question and its nested comments/replies (post_id, comment_id), along with the direct URL of the content item.
•	Core Textual Content: The main body of the post/question (text_content, post_text), titles (title), and the full text of all comments and replies. This forms the primary input for Natural Language Processing.
•	Author Information: Usernames of the content creators (author_name, user_posted, commenter_name).
•	Temporal Data: Timestamps indicating when the content was posted, commented, or replied (timestamp, post_date, comment_date, date_of_reply).
•	Engagement Metrics: Quantitative indicators of popularity or interaction, such as upvotes, scores, views, and shares.
•	Contextual Metadata: Information providing context to the content, including the subreddit name (community_name), question categories, or other platform-specific attributes.
•	Hierarchical Structure: Critical for understanding conversations, the raw data captures the nested relationships between posts, comments, and replies, preserving the conversational flow for later flattening and analysis.
8.3.3	Web Scraping Methods
The data collection process primarily leverages Brightdata's comprehensive suite of web scraping services to efficiently and reliably acquire social media content. This approach minimizes the need for developing and maintaining complex in-house scraping infrastructure, contributing to the project's development efficiency.
The methodology involves two main phases:
•	Link Acquisition: The process begins with identifying relevant social media post URLs. The system utilizes Brightdata's SERP API to perform targeted Google searches (e.g., employing the "site:reddit.com [keyword]" filter) to retrieve a curated list of social media links relevant to specific keywords or topics. For Quora, a unique challenge arises as its content is not consistently indexed by Google; thus, Quora links are typically provided manually by the user within the system's control panel.
•	Content Scraping: Once relevant URLs are identified, Brightdata's specialized Web Scrapers (specifically configured for Reddit and Quora) are invoked. These managed scrapers are designed to handle various complexities inherent in web data extraction, including dynamic content loading, anti-bot measures, and IP rotation. They are configured to navigate the specified URLs and extract the detailed content types as outlined in Section Data Types Collected.
Data Delivery Mechanism: Crucially, upon the completion of a scraping job, Brightdata's scrapers are configured to directly publish the collected data as messages to a Google Pub/Sub topic. This asynchronous delivery mechanism ensures reliable and decoupled transfer of raw content to the system's ingestion pipeline for subsequent processing.
8.3.4	Data Volume
The social listening tool is designed for flexible, on-demand data collection. Users can specify the number of search results to retrieve from the SERP API (which defaults to 20 links but is user-adjustable), and directly provide lists of relevant social media URLs for scraping. While there is no strict limit imposed by the system itself on the number of links a user can request, the practical constraint is defined by Brightdata's web scraper batch limit of 1,000 links per single batch operation.
A typical, targeted batch scraping request, reflecting the immediate analytical needs of a consultant, often involves retrieving content associated with approximately 50 relevant social media posts. This commonly translates to collecting a combined volume of 300 to 500 individual content items (including posts, comments, and replies) per query execution. This flexible, yet typically moderate-scale, acquisition strategy aligns with the system's design for cost-efficient, on-demand insights. The underlying cloud infrastructure and Brightdata's services are inherently scalable, allowing for the processing of much larger data volumes if required for future expansion or extensive research initiatives, up to the defined batch limits.
8.3.5	Ethical and Legal Considerations
The data collection process for the social listening tool adheres to stringent ethical guidelines and legal considerations, particularly concerning the acquisition and use of publicly available social media content.
•	Terms of Service (ToS) Compliance: A thorough review of the Terms of Service for all targeted platforms (e.g., Reddit, Quora) and the data acquisition service (Brightdata) was conducted to ensure compliance with their respective policies. The data collected is limited to publicly accessible content, and operations are conducted in a manner that respects platform guidelines.
•	Data Privacy and Anonymity: The project's focus is on analyzing broad trends, themes, and collective opinions rather than individual user profiling. Data collection is strictly limited to publicly available information. No personally identifiable information (PII) beyond publicly visible usernames (which are treated as pseudonyms for analytical purposes) is actively sought, stored, or processed. Insights generated prioritize aggregate patterns over individual attribution.
•	Responsible Data Use: The collected social media data is intended exclusively for internal strategic analysis by Sense Worldwide. It will be used to enhance client insights and inform strategic recommendations. There is no intent for public redistribution of the raw data, nor for any use that would unfairly disadvantage individuals or violate their privacy expectations.
•	Legal Compliance: The data collection methodology operates under general principles of legal compliance regarding the scraping of publicly available web data. By utilizing a managed third-party service like Brightdata, which typically handles compliance aspects for its scraping operations, and focusing on publicly accessible information for analytical purposes, potential legal complexities are mitigated while ensuring responsible data acquisition.
8.4	DATA ANALYSIS METHODS
This section details the analytical techniques and algorithms employed to transform the collected raw and unified social media data into actionable insights. It outlines the specific methods applied at each stage of the processing pipeline, from text preparation to the generation of embeddings, sentiment scores, topics, and their interpretations. These methods are designed to extract meaningful patterns and insights from unstructured textual data, supporting the strategic objectives of the platform.
8.4.1	Embeddings
Following the preparation of unified social media content, the first analytical step involves the generation of text embeddings. Embeddings are dense numerical vector representations of text (words, phrases, or entire documents) that capture their semantic meaning. They are crucial for advanced text analysis because they transform unstructured textual data into a mathematical format, allowing computational models to perform operations like calculating similarity, clustering, and classification.
In this system, embeddings are generated by BigQuery ML, invoking the text-embedding-004 foundation model served by Google Vertex AI. This process is applied to the primary_text field of each content item from the unified_social_content_items view. The use of this sophisticated pre-trained model allows for direct processing of raw text, as discussed, and ensures that the generated vectors effectively capture the rich contextual and semantic information necessary for subsequent analytical steps like topic modeling and dimensionality reduction.
8.4.2	Topic Modeling (K-Means)
After generating numerical embeddings that represent the semantic content of each social media item, the system proceeds with topic modeling to identify latent themes within the dataset. K-Means clustering is employed for this purpose, operating directly on the generated text embeddings.
K-Means is an unsupervised clustering algorithm that partitions n observations (in this case, content item embeddings) into k distinct clusters, where each observation belongs to the cluster with the nearest mean (centroid). Each cluster, defined by its centroid, represents a coherent topic or theme derived from the content items it contains.
This algorithm was chosen for its:
•	Efficiency and Scalability: K-Means' computational efficiency, combined with its native, built-in implementation within BigQuery ML, makes it highly suitable for processing large datasets directly in the data warehouse, minimizing data movement and optimizing performance.
•	Simplicity and Interpretability: K-Means provides straightforward cluster assignments that form the basis for further, more sophisticated interpretation.
•	Suitability for Embeddings: It performs effectively on dense vector representations like embeddings, particularly when using appropriate distance metrics such as cosine similarity.
The output of the K-Means process is a set of topic IDs, where each content item is assigned to a specific cluster. The interpretation of these numerical clusters into human-understandable topics is a subsequent, crucial step, leveraging advanced AI capabilities.
8.4.3	Sentiment Analysis
To gain deeper insights into the emotional tone and public perception surrounding specific topics and content, the system performs sentiment analysis on the social media data. Sentiment analysis, or opinion mining, is a Natural Language Processing (NLP) technique used to determine the emotional polarity (positive, negative, or neutral) and strength (magnitude) expressed within a piece of text.
For this project, sentiment analysis is applied to the primary_text of each content item. This provides a crucial layer of understanding regarding collective opinions, brand perception, or reactions to specific events or discussions. The analysis leverages a sophisticated, pre-trained NLP model capable of discerning nuances in language to provide accurate sentiment scores, contributing significantly to the actionable insights delivered by the platform.
8.4.4	LLM-based Topic Interpretation (Gemini)
While K-Means clustering effectively groups semantically similar content items into numerical topics, transforming these clusters into human-understandable and actionable insights requires an additional interpretative step. This system leverages the capabilities of a Large Language Model (LLM), specifically Google's Gemini, for automated topic interpretation.
The role of the LLM is to bridge the gap between the purely statistical output of clustering and intuitive human understanding. It operates by analyzing a selection of representative documents (e.g., those mathematically closest to a cluster's centroid) for each identified topic. The LLM then generates concise, human-readable topic titles and brief topic descriptions that summarize the overarching theme of the cluster.
This innovative, LLM-based approach significantly enhances the quality and speed of insight generation compared to traditional manual topic labeling methods. It automates a resource-intensive task, provides contextually rich interpretations, and accelerates the path for consultants to rapidly grasp the essence of newly discovered themes, thereby deriving deeper consumer insights.
8.4.5	Dimensionality Reduction (UMAP)
To facilitate the visual exploration of content relationships and topic clusters, the high-dimensional text embeddings are subjected to dimensionality reduction. Dimensionality reduction is a technique used to reduce the number of features (dimensions) in a dataset while preserving its most important information.

For this project, UMAP (Uniform Manifold Approximation and Projection) is employed. UMAP is a non-linear dimensionality reduction algorithm chosen specifically for its superior ability to preserve both the local (within-cluster proximity) and global (overall arrangement of clusters) structure of the high-dimensional data in a lower-dimensional space (typically 2D or 3D). This is crucial for creating intuitive and accurate scatter plots in the visualization layer.

By projecting the embeddings into a visually navigable space, UMAP enables consultants to intuitively explore the semantic relationships between individual content items, identify the boundaries of topics, and gain a qualitative understanding of the overall data landscape within the interactive dashboard.
8.5	TOOLS AND TECHNOLOGIES
This section enumerates the core tools, technologies, and services utilized in the development and operation of the social listening platform. The selection of these components was guided by principles of scalability, integration, and cost-efficiency, leveraging a modern cloud-native ecosystem to achieve the project's objectives.
8.5.1	Cloud Platform & Services
The social listening platform is built predominantly on Google Cloud Platform (GCP), leveraging its integrated ecosystem of services. GCP was chosen for its robust infrastructure, serverless and managed service offerings, and pay-per-use pricing model, which directly align with the project's objectives for scalability, high availability, and cost-efficiency.
The core GCP services utilized include:
•	Google BigQuery: Serves as the central, serverless, and highly scalable data warehouse for all raw, intermediate, and processed data. It supports complex analytical queries and efficient data storage.
•	BigQuery ML: Extends BigQuery's capabilities by enabling machine learning model creation and execution directly within the data warehouse. It is crucial for generating embeddings, performing K-Means clustering, and invoking external AI models.
•	Google Cloud Functions: Provides serverless compute resources for event-driven tasks such as orchestrating external API calls, handling asynchronous data ingestion, and performing specialized data processing (e.g., UMAP dimensionality reduction).
•	Google Pub/Sub: Functions as a managed, asynchronous messaging service, ensuring reliable and decoupled communication between system components, particularly for receiving scraped data from external sources.
•	Google Vertex AI: Provides access to a suite of machine learning tools and foundation models. Specifically, the Gemini Large Language Model (LLM) via Vertex AI is used for advanced topic interpretation.
•	Google Cloud Natural Language API: A specialized service that provides advanced Natural Language Processing capabilities. It is invoked for performing sentiment analysis on collected social media content.
•	Looker Studio: A free, web-based data visualization tool that connects directly to BigQuery, enabling the creation of interactive dashboards and specialized reports for presenting insights.
•	Google Apps Script: Integrated within Google Sheets, it provides the automation layer for the user interface, translating user actions into triggers for Cloud Functions and orchestrating workflow.
8.5.2	Data Acquisition Services
For the crucial task of extracting raw social media data, the project utilizes Brightdata. This external service provides a comprehensive and managed solution for web data acquisition, significantly reducing the complexity and overhead associated with building and maintaining custom scraping infrastructure.
Brightdata's services employed in this project include:
•	SERP API: Used for programmatic access to search engine results pages, enabling targeted searches (e.g., specific domains like Reddit) to identify relevant social media links.
•	Specialized Web Scrapers: Pre-built and maintained scrapers (e.g., for Reddit and Quora) designed to efficiently extract detailed content (posts, comments, replies) from these platforms. These scrapers handle complexities such as dynamic content, anti-bot measures, and IP rotation.
The selection of Brightdata was strategic due to its reliability, scalability in handling diverse scraping tasks, and its cost-efficiency compared to the continuous investment required for in-house web scraping development and maintenance.
8.5.3	Programming Languages & Libraries
The system's development and custom scripting are primarily executed using Python. Python was selected for its extensive ecosystem of data science libraries, readability, and strong integration capabilities with Google Cloud services.
Key Python libraries utilized in the project include:
•	google-cloud-bigquery: For programmatic interaction with BigQuery, including data loading, querying, and managing datasets from within Cloud Functions.
•	flask: Utilized in Cloud Functions for handling HTTP requests and defining API endpoints, particularly for the orchestration layer.
•	requests: For making HTTP calls to external APIs, including initial interactions with Brightdata.
•	numpy: A fundamental library for numerical operations, supporting data manipulation within Python scripts.
•	pandas: For efficient data manipulation, transformation, and analysis tasks within Python-based components.
•	scikit-learn: Used for general machine learning and data preprocessing tasks performed in Python, prior to BigQuery ML processing.
•	umap-learn: Specifically employed for performing Uniform Manifold Approximation and Projection (UMAP) dimensionality reduction on text embeddings within the dedicated Cloud Function.
These libraries collectively provide the necessary tools for data orchestration, processing, and analytical computations not natively handled by BigQuery ML or other managed services.
8.6	IMPLEMENTATION APPROACH AND PHASES
This section outlines the practical workflow and key phases involved in translating the system's architectural design into a functional, working prototype. The development methodology adopted ensured efficient progress and iterative refinement, facilitating a systematic approach to building the social listening platform.
8.6.1	 Development Workflow
The project followed an iterative and agile-inspired development workflow. This approach allowed for continuous design refinement, rapid prototyping of individual components, and incremental integration. This meant that development tasks were often pursued in parallel, and the boundaries between conceptual "phases" were fluid and overlapping rather than strictly sequential. This iterative process, emphasizing early identification and resolution of issues, ensured flexibility and responsiveness to evolving technical requirements.
8.6.2	Key Implementation Phases
The development process was structured into several distinct yet interconnected phases. While presented sequentially for clarity, it is important to note that these phases often overlapped considerably, reflecting the iterative and parallel nature of the overall development workflow:
•	Phase 1: Foundation & Data Ingestion Setup: This initial phase involved setting up the core Google Cloud Project environment, configuring necessary permissions, and integrating with Brightdata. Key Cloud Functions for SERP search initiation, scraping job orchestration, and raw data delivery into BigQuery were developed and deployed.
•	Phase 2: Data Modeling & Transformation: Focused on defining the logical structure of the data. This included creating the unified_social_content_items BigQuery View for data standardization and flattening, and setting up the embeddings_cache table for pre-computed analytical features. The scheduled MERGE query for incremental embedding and sentiment generation was also established.
•	Phase 3: Advanced Analytics & AI Integration: This phase implemented the core analytical capabilities. BigQuery ML models for K-Means clustering were developed. Integration with Google Vertex AI (for Gemini LLM) and Google Cloud Natural Language API (for sentiment) via BigQuery ML remote models was established. The Cloud Function for UMAP dimensionality reduction was also developed.
•	Phase 4: User Interface & Visualization Development: Involved building the user-facing components. This included developing the Google Sheets control panel (Apps Script) to manage inputs and trigger workflows, and designing the interactive dashboards in Looker Studio for presenting insights.
•	Phase 5: Testing & Refinement: Throughout and upon completion of major development phases, systematic testing was conducted to ensure component functionality, data integrity across the pipeline, and end-to-end system operation. This phase included debugging, performance checks, and initial adjustments based on observed behavior.
8.7	EVALUATION STRATEGY
This section outlines the comprehensive plan for evaluating the developed social listening tool. The evaluation aims to assess its effectiveness, efficiency, and real-world utility in generating consumer insights for Sense Worldwide, thereby demonstrating the successful fulfillment of the project's objectives.
8.7.1	Measuring Utility for Exploratory Research and Specialized Reports
The tool's utility for supporting exploratory research and generating specialized reports will be assessed qualitatively. This involves gathering feedback on the ease with which consultants can gain initial understanding of a topic, identify key themes, and leverage the platform for one-off analytical tasks.
8.7.2	Ease of use and getting results from the tool
TODO
8.7.3	Measuring Contribution to Consumer Insights
The contribution to generating actionable consumer insights will be primarily evaluated through qualitative means. This involves assessing whether the insights derived from the tool lead to "aha!" moments, inform new hypotheses, or directly support strategic recommendations for client engagements.
8.7.4	Measuring Efficiency (Time Savings)
The system's efficiency will be quantitatively measured by assessing time savings. This involves comparing the time required to complete specific social media research tasks (e.g., gathering insights on a new market trend) using the automated tool versus traditional manual desk research methods.
8.7.5	Measuring Cost-Effectiveness
Cost-effectiveness will be quantitatively evaluated by comparing the operational costs of the developed platform against the estimated expenses of manual labor for equivalent tasks or subscription fees for comparable commercial social listening tools. This will involve tracking actual GCP and Brightdata usage costs to demonstrate savings.
8.7.6	Qualitative Feedback Collection
Qualitative data will be systematically collected to capture user experience and perceived value. This will involve conducting semi-structured interviews with data analysts and non-technical consultants from Sense Worldwide, focusing on usability, the clarity and relevance of insights, and overall satisfaction. User testing sessions may also be conducted to observe interactions and gather direct feedback.
9	IMPLEMENTATION
This chapter details the practical implementation of the social listening platform designed in Methodology and System Design. It demonstrates how the proposed architecture and analytical methods were technically realized, showcasing component configuration, integration, and deployment to achieve the system's objectives. Subsequent sections describe the development environment, key implementation details across system layers, and practical challenges encountered during the build process.
9.1	DEVELOPMENT ENVIRONMENT SETUP
The social listening platform was developed within a standardized environment to ensure efficient progress. The primary local development setup utilized Python 3.9 and Visual Studio Code. This environment provided essential tools for script development, local testing, and version control.
For cloud-based development and deployment, a dedicated Google Cloud Platform (GCP) project was established. This central hub hosted all GCP services, including BigQuery, Cloud Functions, Pub/Sub, and Vertex AI. Secure access and resource management were maintained through appropriate Identity and Access Management (IAM) roles and service accounts.
10	REFERENCES
Donkin, R. (2010) The Future of Work. [Online]. Palgrave Macmillan UK. Available from: doi:10.1057/9780230274198.
Sense Worldwide. (2024). The Sense Network - Sense Worldwide. [online] Available at: https://senseworldwide.com/innovate-with-the-sense-network/ [Accessed 8 May 2025]
Nesta (2011) Sense Worldwide « OpenBusiness. [Online]. 2011. Available from: https://web.archive.org/web/20110830101558/http://www.openbusiness.cc/2010/01/06/sense-worldwide/ [Accessed: 31 July 2021].
Peffers, K. et al. (2007) ‘A Design Science Research Methodology for Information Systems Research’, Journal of Management Information Systems, 24(3), pp. 45–77. doi: 10.2753/MIS0742-1222240302.
Hevner, A., March, S., Park, J. and Ram, S. (2004). Design Science in Information Systems Research. MIS Quarterly, [online] 28(1), pp.75–105. doi:https://doi.org/10.2307/25148625.
11	APPENDIX
11.1	UNIFIED_SOCIAL_CONTENT_ITEMS VIEW
This appendix provides the complete SQL definition for the unified_social_content_items BigQuery View. This view is a crucial component of the data model, responsible for flattening hierarchical social media content and standardizing schemas from various sources into a single, unified dataset for downstream analytical processing.
CREATE OR REPLACE VIEW `social-listening-sense.social_listening_data.unified_social_content_items` AS

WITH
  -- CTE for Reddit Posts: Rely on t.post_id for stable_post_id, filter out rows where post_id is NULL.
  reddit_posts_base AS (
    SELECT
        t.post_id,
        t.url,
        t.user_posted,
        t.title,
        t.description,
        t.num_upvotes,
        t.date_posted,
        t.community_name,
        t.photos,
        t.videos,
        t.timestamp,
        t.comments,
        t.snapshot_id,
        t.error_code,
        t.error,
        t.warning_code,
        t.warning,
        t.post_id AS stable_post_id
    FROM
      `social-listening-sense.social_listening_data.reddit_data` AS t
    WHERE
      t.post_id IS NOT NULL
      AND (t.title IS NOT NULL OR t.description IS NOT NULL)
  ),

  -- CTE for Reddit Comments: Generate stable comment_item_id without UUID.
  reddit_comments_base AS (
    SELECT
        p.stable_post_id,
        c.url AS comment_url,
        c.user_commenting,
        c.comment,
        c.num_upvotes,
        c.date_of_comment,
        c.replies,
        -- Generate a stable ID for the comment using deterministic fields
        CONCAT('reddit_comment_gen_', FARM_FINGERPRINT(
            CONCAT(
                COALESCE(p.stable_post_id, ''),
                COALESCE(c.user_commenting, ''),
                FORMAT_TIMESTAMP('%Y%m%d%H%M%S%F', COALESCE(c.date_of_comment, CAST('1970-01-01 00:00:00 UTC' AS TIMESTAMP))),
                COALESCE(c.comment, '')
            )
        )) AS stable_comment_id
    FROM
      reddit_posts_base AS p,
      UNNEST(p.comments) AS c
    WHERE
      c.comment IS NOT NULL
  ),

  -- CTE for Reddit Replies: Generate stable reply_item_id without UUID.
  reddit_replies_base AS (
    SELECT
        c.stable_post_id,
        c.stable_comment_id,
        r.user_url,
        r.user_replying,
        r.num_upvotes,
        r.date_of_reply,
        -- Using r.user_replying as content
        r.user_replying AS reply_content_placeholder,
        -- Generate a stable ID for the reply using deterministic fields
        CONCAT('reddit_reply_gen_', FARM_FINGERPRINT(
            CONCAT(
                COALESCE(c.stable_comment_id, ''),
                COALESCE(r.user_replying, ''),
                FORMAT_TIMESTAMP('%Y%m%d%H%M%S%F', COALESCE(r.date_of_reply, CAST('1970-01-01 00:00:00 UTC' AS TIMESTAMP))),
                COALESCE(r.user_replying, '') --  Used for hash
            )
        )) AS stable_reply_id
    FROM
      reddit_comments_base AS c,
      UNNEST(c.replies) AS r
    WHERE
      r.user_replying IS NOT NULL -- Filtering on user_replying,
  ),

  -- CTE for Quora Questions: Rely on t.post_id for stable_post_id, filter out rows where post_id is NULL.
  quora_questions_base AS (
    SELECT
        t.post_id,
        t.url,
        t.author_name,
        t.title,
        t.post_text,
        t.upvotes,
        t.post_date,
        t.pictures_urls,
        t.videos_urls,
        t.top_comments,
        t.timestamp,
        t.snapshot_id,
        t.post_id AS stable_post_id
    FROM
      `social-listening-sense.social_listening_data.quora_data` AS t
    WHERE
      t.post_id IS NOT NULL
      AND (t.title IS NOT NULL OR t.post_text IS NOT NULL)
  ),

  -- CTE for Quora Comments: Generate stable comment_item_id.
  quora_comments_base AS (
    SELECT
        q.stable_post_id,
        q.url AS question_url,
        c.commenter_name,
        c.comment,
        c.comment_date,
        c.replys,
        -- Generate a stable ID for the comment
        CONCAT('quora_comment_gen_', FARM_FINGERPRINT(
            CONCAT(
                COALESCE(q.stable_post_id, ''),
                COALESCE(c.commenter_name, ''),
                FORMAT_TIMESTAMP('%Y%m%d%H%M%S%F', COALESCE(c.comment_date, CAST('1970-01-01 00:00:00 UTC' AS TIMESTAMP))),
                COALESCE(c.comment, '')
            )
        )) AS stable_comment_id
    FROM
      quora_questions_base AS q,
      UNNEST(q.top_comments) AS c
    WHERE
      c.comment IS NOT NULL
  ),

  -- CTE for Quora Replies: Generate stable reply_item_id.
  quora_replies_base AS (
    SELECT
        c.stable_post_id,
        c.stable_comment_id,
        r.commenter_name,
        r.comment,
        r.comment_date,
        -- Generate a stable ID for the reply
        CONCAT('quora_reply_gen_', FARM_FINGERPRINT(
            CONCAT(
                COALESCE(c.stable_comment_id, ''),
                COALESCE(r.commenter_name, ''),
                FORMAT_TIMESTAMP('%Y%m%d%H%M%S%F', COALESCE(r.comment_date, CAST('1970-01-01 00:00:00 UTC' AS TIMESTAMP))),
                COALESCE(r.comment, '')
            )
        )) AS stable_reply_id
    FROM
      quora_comments_base AS c,
      UNNEST(c.replys) AS r
    WHERE
      r.comment IS NOT NULL
  )

--- Final UNION ALL for the Unified View ---
SELECT * FROM (
  SELECT
      'Reddit' AS source,
      'post' AS content_type,
      t.stable_post_id AS content_item_id,
      CAST(NULL AS STRING) AS parent_content_item_id,
      t.stable_post_id AS top_level_post_id,
      t.url AS content_item_url,
      t.user_posted AS author_username,
      t.title AS primary_text,
      t.description AS full_text_context,
      CAST(t.num_upvotes AS INT64) AS engagement_score,
      t.date_posted AS content_timestamp,
      t.community_name AS community_or_channel_name,
      ARRAY<STRING>[] AS hashtags,
      ARRAY<STRING>[] AS mentions,
      t.photos AS media_urls,
      t.videos AS video_urls,
      t.timestamp AS record_load_timestamp,
      t.snapshot_id,
      ROW_NUMBER() OVER(PARTITION BY t.stable_post_id ORDER BY t.date_posted DESC) as rn
  FROM
      reddit_posts_base AS t
)
WHERE rn = 1

UNION ALL

SELECT * FROM (
  SELECT
      'Reddit' AS source,
      'comment' AS content_type,
      c.stable_comment_id AS content_item_id,
      c.stable_post_id AS parent_content_item_id,
      c.stable_post_id AS top_level_post_id,
      c.comment_url AS content_item_url,
      c.user_commenting AS author_username,
      c.comment AS primary_text,
      CONCAT(COALESCE(rp.title, ''), ' ', COALESCE(rp.description, ''), ' ', COALESCE(c.comment, '')) AS full_text_context,
      CAST(c.num_upvotes AS INT64) AS engagement_score,
      c.date_of_comment AS content_timestamp,
      rp.community_name AS community_or_channel_name,
      ARRAY<STRING>[] AS hashtags,
      ARRAY<STRING>[] AS mentions,
      ARRAY<STRING>[] AS media_urls,
      ARRAY<STRING>[] AS video_urls,
      rp.timestamp AS record_load_timestamp,
      rp.snapshot_id,
      ROW_NUMBER() OVER(PARTITION BY c.stable_comment_id ORDER BY c.date_of_comment DESC, rp.date_posted DESC) as rn
  FROM
      reddit_comments_base AS c
  JOIN
      reddit_posts_base AS rp
  ON
      c.stable_post_id = rp.stable_post_id
)
WHERE rn = 1

UNION ALL

SELECT * FROM (
  SELECT
      'Reddit' AS source,
      'reply' AS content_type,
      r.stable_reply_id AS content_item_id,
      r.stable_comment_id AS parent_content_item_id,
      r.stable_post_id AS top_level_post_id,
      r.user_url AS content_item_url,
      r.user_replying AS author_username,
      r.reply_content_placeholder AS primary_text, 
      CONCAT(COALESCE(rp.title, ''), ' ', COALESCE(rp.description, ''), ' ', COALESCE(rc.comment, ''), ' ', COALESCE(r.reply_content_placeholder, '')) AS full_text_context, 
      CAST(r.num_upvotes AS INT64) AS engagement_score,
      r.date_of_reply AS content_timestamp,
      rp.community_name AS community_or_channel_name,
      ARRAY<STRING>[] AS hashtags,
      ARRAY<STRING>[] AS mentions,
      ARRAY<STRING>[] AS media_urls,
      ARRAY<STRING>[] AS video_urls,
      rp.timestamp AS record_load_timestamp,
      rp.snapshot_id,
      ROW_NUMBER() OVER(PARTITION BY r.stable_reply_id ORDER BY r.date_of_reply DESC, rc.date_of_comment DESC, rp.date_posted DESC) as rn
  FROM
      reddit_replies_base AS r
  JOIN
      reddit_comments_base AS rc
  ON
      r.stable_comment_id = rc.stable_comment_id
  JOIN
      reddit_posts_base AS rp
  ON
      r.stable_post_id = rp.stable_post_id
)
WHERE rn = 1

UNION ALL

SELECT * FROM (
  SELECT
      'Quora' AS source,
      'post' AS content_type,
      t.stable_post_id AS content_item_id,
      CAST(NULL AS STRING) AS parent_content_item_id,
      t.stable_post_id AS top_level_post_id,
      t.url AS content_item_url,
      t.author_name AS author_username,
      t.title AS primary_text,
      t.post_text AS full_text_context,
      CAST(t.upvotes AS INT64) AS engagement_score,
      t.post_date AS content_timestamp,
      CAST(NULL AS STRING) AS community_or_channel_name,
      ARRAY<STRING>[] AS hashtags,
      ARRAY<STRING>[] AS mentions,
      t.pictures_urls AS media_urls,
      CASE WHEN t.videos_urls IS NOT NULL THEN [t.videos_urls] ELSE ARRAY<STRING>[] END AS video_urls,
      t.timestamp AS record_load_timestamp,
      t.snapshot_id,
      ROW_NUMBER() OVER(PARTITION BY t.stable_post_id ORDER BY t.post_date DESC) as rn
  FROM
      quora_questions_base AS t
)
WHERE rn = 1

UNION ALL

SELECT * FROM (
  SELECT
      'Quora' AS source,
      'comment' AS content_type,
      c.stable_comment_id AS content_item_id,
      c.stable_post_id AS parent_content_item_id,
      c.stable_post_id AS top_level_post_id,
      q.url AS content_item_url,
      c.commenter_name AS author_username,
      c.comment AS primary_text,
      CONCAT(COALESCE(q.title, ''), ' ', COALESCE(q.post_text, ''), ' ', COALESCE(c.comment, '')) AS full_text_context,
      CAST(NULL AS INT64) AS engagement_score,
      c.comment_date AS content_timestamp,
      CAST(NULL AS STRING) AS community_or_channel_name,
      ARRAY<STRING>[] AS hashtags,
      ARRAY<STRING>[] AS mentions,
      ARRAY<STRING>[] AS media_urls,
      ARRAY<STRING>[] AS video_urls,
      q.timestamp AS record_load_timestamp,
      q.snapshot_id,
      ROW_NUMBER() OVER(PARTITION BY c.stable_comment_id ORDER BY c.comment_date DESC, q.post_date DESC) as rn
  FROM
      quora_comments_base AS c
  JOIN
      quora_questions_base AS q
  ON
      c.stable_post_id = q.stable_post_id
)
WHERE rn = 1

UNION ALL

SELECT * FROM (
  SELECT
      'Quora' AS source,
      'reply' AS content_type,
      r.stable_reply_id AS content_item_id,
      r.stable_comment_id AS parent_content_item_id,
      r.stable_post_id AS top_level_post_id,
      q.url AS content_item_url,
      r.commenter_name AS author_username,
      r.comment AS primary_text,
      CONCAT(COALESCE(q.title, ''), ' ', COALESCE(q.post_text, ''), ' ', COALESCE(qc.comment, ''), ' ', COALESCE(r.comment, '')) AS full_text_context,
      CAST(NULL AS INT64) AS engagement_score,
      r.comment_date AS content_timestamp,
      CAST(NULL AS STRING) AS community_or_channel_name,
      ARRAY<STRING>[] AS hashtags,
      ARRAY<STRING>[] AS mentions,
      ARRAY<STRING>[] AS media_urls,
      ARRAY<STRING>[] AS video_urls,
      q.timestamp AS record_load_timestamp,
      q.snapshot_id,
      ROW_NUMBER() OVER(PARTITION BY r.stable_reply_id ORDER BY r.comment_date DESC, qc.comment_date DESC, q.post_date DESC) as rn
  FROM
      quora_replies_base AS r
  JOIN
      quora_comments_base AS qc
  ON
      r.stable_comment_id = qc.stable_comment_id
  JOIN
      quora_questions_base AS q
  ON
      r.stable_post_id = q.stable_post_id
)
WHERE rn = 1


